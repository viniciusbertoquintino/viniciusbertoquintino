{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "id": "6eGKcwYtdWVu",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAI\n",
    "from langchain.cache import InMemoryCache,SQLiteCache\n",
    "from langchain.globals import set_llm_cache\n",
    "import os\n",
    "import json\n",
    "import hashlib\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NEg_0v_VpqeA"
   },
   "outputs": [],
   "source": [
    "with open('config.yaml', 'r') as config_file:\n",
    "    config = yaml.safe_load(config_file)\n",
    "os.environ['OPENAI_API_KEY'] = config['OPENAI_API_KEY']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TftZH6m-pSe4"
   },
   "source": [
    "# Memória"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8VOSdK2vdfeO"
   },
   "outputs": [],
   "source": [
    "openai = OpenAI(model_name='gpt-3.5-turbo-instruct')\n",
    "set_llm_cache(InMemoryCache())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TCOBpmFBdfho",
    "outputId": "ccdade8c-6e16-41b2-f4e1-495112be3eda"
   },
   "outputs": [],
   "source": [
    "prompt = 'Me diga em poucas palavras que foi Carl Sagan.'\n",
    "response1 = openai.invoke(prompt)\n",
    "print(\"Primeira resposta (API chamada):\", response1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-SuwTClBdfkU",
    "outputId": "aed30081-9aa1-456c-a948-f5b118a431f9"
   },
   "outputs": [],
   "source": [
    "response2 = openai.invoke(prompt)\n",
    "print(\"Segunda resposta (usando cache):\", response2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tTtYinh-pcj7"
   },
   "source": [
    "# Disco / Banco de Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dp4gExg3lbAH"
   },
   "outputs": [],
   "source": [
    "set_llm_cache(SQLiteCache(database_path=\"openai_cache.db\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Zh0m5jo8lggv"
   },
   "outputs": [],
   "source": [
    "prompt = 'Me diga em poucas palavras quem foi Neil Armstrong.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gAeoflbzlbDK",
    "outputId": "ef4c5f9f-0fed-431b-96be-ef30eed34370"
   },
   "outputs": [],
   "source": [
    "response1 = openai.invoke(prompt)\n",
    "print(\"Primeira resposta (API chamada):\", response1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XNlIF1YklbF4",
    "outputId": "621f0482-030d-4dc1-a2ca-e020168b7d51"
   },
   "outputs": [],
   "source": [
    "response2 = openai.invoke(prompt)\n",
    "print(\"Segunda resposta (usando cache):\", response2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gCzytB3cph2u"
   },
   "source": [
    "# Personalizado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hQdCdSoAps26"
   },
   "outputs": [],
   "source": [
    "class SimpleDiskCache:\n",
    "    def __init__(self, cache_dir='cache_dir'):\n",
    "        self.cache_dir = cache_dir\n",
    "        os.makedirs(self.cache_dir, exist_ok=True)\n",
    "\n",
    "    def _get_cache_path(self, key):\n",
    "        hashed_key = hashlib.md5(key.encode()).hexdigest() #hasg cria nome de arquivo único\n",
    "        return os.path.join(self.cache_dir, f\"{hashed_key}.json\")\n",
    "\n",
    "    def lookup(self, key, llm_string):\n",
    "        cache_path = self._get_cache_path(key)\n",
    "        if os.path.exists(cache_path):\n",
    "            with open(cache_path, 'r') as f:\n",
    "                return json.load(f)\n",
    "        return None\n",
    "\n",
    "    def update(self, key, value, llm_string):\n",
    "        cache_path = self._get_cache_path(key)\n",
    "        with open(cache_path, 'w') as f:\n",
    "            json.dump(value, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wjp02OMnps59"
   },
   "outputs": [],
   "source": [
    "cache = SimpleDiskCache()\n",
    "set_llm_cache(cache)\n",
    "prompt = 'Me diga em poucas palavras quem foi Neil Degrasse Tyson.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jD6mmnObps9C"
   },
   "outputs": [],
   "source": [
    "def invoke_with_cache(llm, prompt, cache):\n",
    "    cached_response = cache.lookup(prompt, \"\")\n",
    "    if cached_response:\n",
    "        print(\"Usando cache:\")\n",
    "        return cached_response\n",
    "\n",
    "    response = llm.invoke(prompt)\n",
    "    cache.update(prompt, response, \"\")\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3yzsKC8CptAK",
    "outputId": "80e6f397-96cd-4058-cca7-679cc3158496"
   },
   "outputs": [],
   "source": [
    "response1 = invoke_with_cache(openai, prompt, cache)\n",
    "response_text1 = response1.replace('\\n', ' ') \n",
    "\n",
    "print(\"Primeira resposta (API chamada):\", response_text1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "editable": true,
    "id": "2CTyn8qtlbJB",
    "outputId": "b2695de3-0cd5-4cdc-afdc-3e2363464d6b",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "response2 = invoke_with_cache(openai, prompt, cache)\n",
    "response_text2 = response2.replace('\\n', ' ')  \n",
    "\n",
    "print(\"Segunda resposta (usando cache):\", response_text2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
